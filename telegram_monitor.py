import os
import sys
import re
import time
import json
from urllib.parse import urljoin, urlparse

import requests
from bs4 import BeautifulSoup

# ===== å®šæ•°ãƒ»ç’°å¢ƒå¤‰æ•° =====
TELEGRAM_BOT_TOKEN = os.environ.get("TELEGRAM_BOT_TOKEN")
TELEGRAM_CHAT_ID = os.environ.get("TELEGRAM_CHAT_ID")
TARGET_URL = os.environ.get("TARGET_URL")
GITHUB_EVENT_NAME = os.environ.get("GITHUB_EVENT_NAME")

# å¿…é ˆãƒã‚§ãƒƒã‚¯
missing = []
if not TELEGRAM_BOT_TOKEN: missing.append("TELEGRAM_BOT_TOKEN")
if not TELEGRAM_CHAT_ID: missing.append("TELEGRAM_CHAT_ID")
if not TARGET_URL: missing.append("TARGET_URL")

if missing:
    print(f"Missing environment variables: {', '.join(missing)}")
    sys.exit(1)

# URLãƒªã‚¹ãƒˆåŒ–
url_list = [u.strip() for u in TARGET_URL.split(",") if u.strip()]

# User-Agent
headers = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/122.0.0.0 Safari/537.36"
    )
}

# 1å›ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³å®Ÿè¡Œã§é€ä¿¡æ¸ˆã¿ã®IDã‚’è¨˜éŒ²
sent_post_ids = set()

# URLæ¤œçŸ¥ç”¨ æ­£è¦è¡¨ç¾
URL_PATTERN = re.compile(
    r"https?://[\w/:%#\$&\?\(\)~\.=\+\-]+",
    re.IGNORECASE
)

# ===== çŠ¶æ…‹ç®¡ç† (state) =====
def get_board_id(url: str) -> str:
    path = urlparse(url).path.rstrip("/")
    return path.split("/")[-1] if path else "default"

def load_last_post_id(board_id: str):
    fname = f"last_post_id_{board_id}.txt"
    if not os.path.exists(fname): return None
    try:
        with open(fname, "r", encoding="utf-8") as f:
            content = f.read().strip()
            return int(content) if content else None
    except Exception: return None

def save_last_post_id(board_id: str, post_id: int):
    fname = f"last_post_id_{board_id}.txt"
    with open(fname, "w", encoding="utf-8") as f:
        f.write(str(post_id))

# ===== ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ =====
def extract_urls(text: str):
    found = URL_PATTERN.findall(text)
    unique_urls = sorted(list(set(found)))
    filtered_urls = []
    for url in unique_urls:
        parsed = urlparse(url)
        path = parsed.path.rstrip("/")
        last_segment = path.split("/")[-1] if "/" in path else ""
        if "disp" in url or "upup.be" in url:
            filtered_urls.append(url)
            continue
        if last_segment.isdigit(): continue
        filtered_urls.append(url)
    return filtered_urls

# ===== Telegramé€ä¿¡ãƒ­ã‚¸ãƒƒã‚¯ =====
def send_telegram_media_group(board_name, board_id, post_id, posted_at, body_text, target_post_url, media_urls):
    """
    ãƒ¡ãƒ‡ã‚£ã‚¢ã‚’ã‚°ãƒ«ãƒ¼ãƒ—ï¼ˆã‚¢ãƒ«ãƒãƒ ï¼‰ã¨ã—ã¦é€ä¿¡ã—ã€
    ãã®ç›´å¾Œã«è©³ç´°æƒ…å ±ã¨ã€Œãƒ–ãƒ©ã‚¦ã‚¶ã§é–‹ãã€ãƒœã‚¿ãƒ³ã‚’é€ä¿¡ã™ã‚‹ã€‚
    """
    print(f"      [DEBUG] Telegramã¸é€ä¿¡ã‚’è©¦ã¿ã¾ã™... (Media: {len(media_urls)})")
    
    # 1. ãƒ¡ãƒ‡ã‚£ã‚¢ã®æº–å‚™ (æœ€å¤§10æš)
    media_group = []
    processed_count = 0
    
    for m_url in media_urls:
        if processed_count >= 10: break
        
        parsed = urlparse(m_url)
        file_id = parsed.path.rstrip("/").split("/")[-1]
        d_char = parsed.netloc.split('.')[0]
        # cdnX.5chan.jp å½¢å¼ã¸ã®è£œæ­£
        base_netloc = parsed.netloc if d_char.startswith("cdn") else f"cdn{d_char}.5chan.jp"

        # è©¦è¡ŒURLãƒªã‚¹ãƒˆï¼ˆç”»åƒå„ªå…ˆ -> å‹•ç”»ï¼‰
        attempt_urls = [
            f"https://{base_netloc}/file/plane/{file_id}.jpg",
            f"https://{base_netloc}/file/{file_id}.mp4",
            f"https://{base_netloc}/file/plane/{file_id}.png",
            f"https://{base_netloc}/file/{file_id}.gif"
        ]
        # å…ƒã®URLã«æ‹¡å¼µå­ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã¯æœ€å„ªå…ˆ
        if "." in file_id: attempt_urls.insert(0, m_url)

        for target_download_url in attempt_urls:
            try:
                # HEADãƒªã‚¯ã‚¨ã‚¹ãƒˆã§å­˜åœ¨ç¢ºèª
                r = requests.head(target_download_url, headers=headers, timeout=10)
                if r.status_code == 200:
                    # Content-Type ã¾ãŸã¯æ‹¡å¼µå­ã‹ã‚‰ç¨®åˆ¥åˆ¤æ–­
                    content_type = r.headers.get('Content-Type', '').lower()
                    ext = target_download_url.split('.')[-1].lower()
                    
                    if "video" in content_type or ext in ["mp4", "mov", "webm"]:
                        media_type = "video"
                    else:
                        media_type = "photo"
                        
                    media_group.append({"type": media_type, "media": target_download_url})
                    processed_count += 1
                    break
            except: continue

    # 2. ãƒ¡ãƒ‡ã‚£ã‚¢ã‚°ãƒ«ãƒ¼ãƒ—ã®é€ä¿¡
    if media_group:
        send_group_url = f"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/sendMediaGroup"
        # æœ€åˆã®ãƒ¡ãƒ‡ã‚£ã‚¢ã«ã®ã¿ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‚’ã¤ã‘ã‚‹ã“ã¨ã‚‚å¯èƒ½ã§ã™ãŒã€åˆ¥é€”ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é€ã‚‹ãŸã‚ã“ã“ã§ã¯é€ä¿¡ã®ã¿
        requests.post(send_group_url, data={"chat_id": TELEGRAM_CHAT_ID, "media": json.dumps(media_group)})

    # 3. ãƒ†ã‚­ã‚¹ãƒˆã¨ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³ãƒœã‚¿ãƒ³ã®é€ä¿¡
    # å†’é ­300æ–‡å­—ç¨‹åº¦ã‚’å¼•ç”¨
    summary_text = body_text[:300] + ("..." if len(body_text) > 300 else "")
    
    message_text = (
        f"<b>ã€{board_name}ã€‘</b>\n"
        f"æŠ•ç¨¿ç•ªå·: #{post_id}\n"
        f"æŠ•ç¨¿æ—¥æ™‚: {posted_at}\n\n"
        f"{summary_text}"
    )
    
    keyboard = {
        "inline_keyboard": [[
            {"text": "ğŸŒ ãƒ–ãƒ©ã‚¦ã‚¶ã§è©³ç´°ã‚’ç¢ºèª", "url": target_post_url}
        ]]
    }
    
    send_msg_url = f"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/sendMessage"
    payload = {
        "chat_id": TELEGRAM_CHAT_ID,
        "text": message_text,
        "parse_mode": "HTML", # å¤ªå­—ã‚’æœ‰åŠ¹ã«ã™ã‚‹
        "reply_markup": json.dumps(keyboard)
    }
    
    try:
        resp = requests.post(send_msg_url, data=payload)
        if resp.status_code == 200:
            print(f"      [SUCCESS] æŠ•ç¨¿#{post_id} ã®é€ä¿¡å®Œäº†ã€‚")
        else:
            print(f"      [ERROR] Telegramé€ä¿¡å¤±æ•—: {resp.text}")
    except Exception as e:
        print(f"      [ERROR] é€šä¿¡ã‚¨ãƒ©ãƒ¼: {e}")

# ===== ãƒ¡ã‚¤ãƒ³å‡¦ç† =====
for target in url_list:
    board_id = get_board_id(target)
    print(f"--- Checking board: {board_id} ---")
    try:
        resp = requests.get(target, headers=headers, timeout=15)
        resp.raise_for_status()
    except Exception as e:
        print(f" [ERROR] ãƒœãƒ¼ãƒ‰èª­ã¿è¾¼ã¿å¤±æ•— ({target}): {e}")
        continue

    soup = BeautifulSoup(resp.text, "html.parser")
    
    # æ²ç¤ºæ¿ã‚¿ã‚¤ãƒˆãƒ«å–å¾—
    board_name = soup.title.string.split("-")[0].strip() if soup.title else board_id
    
    articles = soup.select("article.resentry")
    if not articles: continue

    # æœ€æ–°ã®ä»¶æ•°ã‚’å¯¾è±¡ã¨ã™ã‚‹ï¼ˆæ—¢èª­IDã‚ˆã‚Šå¤§ãã„ã‚‚ã®ã‚’ã™ã¹ã¦å–å¾—ï¼‰
    last_post_id = load_last_post_id(board_id)
    newest_post_id = last_post_id if last_post_id else 0
    
    # é€†é †ï¼ˆå¤ã„é †ï¼‰ã«å‡¦ç†ã—ã¦æ–°ç€ã‚’æ¼ã‚‰ã•ãªã„
    for article in articles:
        eno_tag = article.select_one("span.eno a")
        if eno_tag is None: continue 
        try:
            post_id = int("".join(filter(str.isdigit, eno_tag.get_text(strip=True))))
        except: continue

        if last_post_id is not None and post_id <= last_post_id:
            continue
            
        if post_id in sent_post_ids: continue
        
        # newest_post_id ã®æ›´æ–°
        if post_id > newest_post_id: newest_post_id = post_id
        
        print(f"  -> [NEW] æŠ•ç¨¿#{post_id} ã‚’å‡¦ç†ä¸­...")
        time_tag = article.select_one("time.date")
        posted_at = time_tag.get_text(strip=True) if time_tag else "N/A"
        comment_div = article.select_one("div.comment")
        body_text = comment_div.get_text("\n", strip=True) if comment_div else ""

        media_urls = []
        
        # æœ¬æ–‡ã‹ã‚‰ã®æŠ½å‡º
        urls_in_body = extract_urls(body_text)
        for u in urls_in_body:
            if "disp" in u or "upup.be" in u: media_urls.append(u)

        # ã‚µãƒ ãƒã‚¤ãƒ«ãƒªã‚¹ãƒˆã‹ã‚‰ã®æŠ½å‡º
        thumblist = article.select(".filethumblist li")
        for li in thumblist:
            a_tag = li.select_one("a[href]")
            if a_tag:
                abs_url = urljoin(target, a_tag.get("href"))
                media_urls.append(abs_url)

        if not media_urls:
            print(f"  -> æŠ•ç¨¿#{post_id} ã¯ç”»åƒ/å‹•ç”»ãŒãªã„ãŸã‚ã‚¹ã‚­ãƒƒãƒ—ã€‚")
            continue

        # æŠ•ç¨¿ã¸ã®ç›´æ¥URLï¼ˆæ²ç¤ºæ¿URLã«ç•ªå·ã‚’ä»˜åŠ ï¼‰
        # ã‚¯ã‚¨ãƒªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ ?from=new2 ãªã©ãŒã¤ã„ã¦ã„ã‚‹å ´åˆã‚’è€ƒæ…®
        base_target = target.split('?')[0].rstrip('/')
        target_post_url = f"{base_target}/{post_id}"
        
        send_telegram_media_group(
            board_name, board_id, post_id, posted_at, body_text, 
            target_post_url, list(dict.fromkeys(media_urls))
        )
        sent_post_ids.add(post_id)

    if newest_post_id > (last_post_id if last_post_id else 0):
        save_last_post_id(board_id, newest_post_id)
